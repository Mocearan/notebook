

# Image

---

​		图像是由自发光或反射光源进入人眼而成像的。所以图像有两种成像原理：自发光物体的RGB色彩空间，和基于人眼成像原理的YUV色彩空间。



## 参考

[图像格式：常见图像格式RAW, RGB, YUV&&图像格式的解析、格式转换和看图软件_raw图片编码-CSDN博客](https://blog.csdn.net/weixin_45264425/article/details/132626509)

[FFMPEG 内部YUV转RGB过程 - 洛笔达 - 博客园 (cnblogs.com)](https://www.cnblogs.com/luoyinjie/p/13669368.html)



## 图像的物理性质

- 光源成像：白光能被分解为三原色，红光（R）、绿光（G）、蓝光（B），即RGB。等量的三原色光叠加会变为白光
  - 电子设备显像是自发光的，不同于其他自然物体是由于反射日光
  -  RGB主要用于电子显示设备（如电脑显示器、电视、数字摄影等），因为这些设备的发光原理与RGB模型相符
- 人眼成像：人眼对亮度变化更敏感，对色度变化感知较弱
  - RGB模式下的图像数据通常较大，特别是对于高分辨率视频，这可能导致带宽需求很高
  - YUV将亮度分量与色度分量分开，UV可以采用色度子采样来减少数据量
    - 保持亮度分量的高分辨率的同时降低色度分量的分辨率
    - 适合于视频流媒体、电视广播、视频压缩（如MPEG、H.264）等需要处理和传输大量数据的场景
  - 人的视网膜有对红、绿、蓝颜色敏感程度不同的三种锥体细胞
    - 红、绿和蓝三种锥体细胞对不同频率的光的感知程度不同，对不同亮度的感知程度也不同
      - 红、绿和蓝三种锥体细胞对不同频率的光的感知程度不同，对不同亮度的感知程度也不同。

​		所以电子设备展示图像时有两个概念，一个是电子设备的物理像素块（RGB），而是图像数据的逻辑像素块（YUV / RGB）。电子设备展示图像时，图像数据的逻辑像素块经过驱动程序的转换让电子设备的物理像素块显示图像。

> 如果程序渲染时没有显式将YUV转换为RGB，意味着显卡或其他渲染组件隐式进行了转换。





### RGB:  电子设备图像表示

​		显示器(RGB)的每个像素点背后都有三个发光二极管组成，即三个通道，每个通道按照位深，显示不等量的三原色，这些色点紧邻且小人眼无法分辨而将其混合，从而构成该像素点的颜色。

​		RGB表示上，除了红绿蓝三个色彩通道，还有一个透明度通道（alpha分量， ARGB）

​		每个通道上，按照一定的规则来定义对应颜色的分级。RGB通道的分级数值表示组成一个像素点的数据格式

- 浮点表示：`0/0~10`，OpenGL ES中每个通道点的表示使用这种方式

- 整数表示：`RGB_8888`,``0~255``或``00~FF``,8位表示一个分量通道上的值。即8位位深度

  - 一些平台上采用其他的整数表示方式

    - 如Android采用`RGB_565`共`16bit`来表示一个像素

    - `RGB_8888` 表示的`1280X720`的图像大小为：`1280 x 720 x 4byte=3.516MB`

      > 这也是位图（bitmap）在内存中所占用的大小，所以每一张图像的裸数据都是很大的

​		RGB像素点中各个分量通常是按照顺序排列的，但有些图像处理要转换成其他的顺序，`opencv`经常转换成BGR的排列方式。

![image-20220907235455776](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220907235455776.png)

		一般称24bit以上位深的色彩为真彩色，当然还有采用30bit、36bit、42bit的。使用的色彩代码越长，同样像素的文件的文件大小也就相应的成幂次级增长。使用超过16位以上的色彩文件在普通的显示器，尤其是液晶显示器上看不出任何区别，原因是液晶显示器本身不能显示出那么多的色彩。但是对于彩色印刷就非常有用，因为油墨的点非常的细，同时由于印刷尺幅的放大原因， 更大的文件可以在印刷的时候呈现出更细腻的层次和细节。

![img](https://raw.githubusercontent.com/Mocearan/picgo-server/main/ed2a79d16be7aa56aea066d248e04421.png)



### YUV:  数字图像表示

​		人眼对亮度敏感而对色度不敏感。因而可以将亮度信息和色度信息分离，以这样“欺骗”人的眼睛的手段来节省空间，从而适合于图像处理领域，从而提高压缩效率。

​		与 RGB 相比，YUV 在响应亮度信息方面更为准确，同时去除了人眼感知中色度无关的特性，使得相对于 RGB，它具有更高的压缩性能，更适合于视频传输或存储。

- Y，明亮度（Luminance或Luma），也称灰阶值

  - 亮度是通过RGB输入信号来建立的概念
  - 方法是将RGB信号的特定部分叠加到一起。

- U / V，色度（Chrominance或Chroma）

  - U表示蓝色色度的信息，V表示红色色度的信息

    - 绿色的色度信息通常是通过蓝色（U）和红色（V）的差值来表示的。

      > 因为人眼对绿色的感知更为敏感，所以通过差值编码可以更有效地表示绿色色度信息，同时减少数据量和保持图像质量。

  - 描述影像的色彩（Cr）及饱和度（Cb），用于指定像素的颜色

  - Cr反映了RGB输入信号红色部分与RGB信号亮度值之间的差异

  - Cb反映的则是RGB输入信号蓝色部分与RGB信号亮度值之间的差异

​		用灰度图来描述图像亮度变化，然后再灰度图上叠加色度图来渲染颜色。同时降低色度的采样率而不会对图像质量影响太大，降低了视频信号传输时对频宽（带宽）的要求。

- 如果只有Y信号分量而没有U、V分量，那么这样表示的图像就是黑白灰度图像。

  - 黑白电视的原理是只有亮度信号
  - 使用YUV能够兼容黑白电视

- 分离亮度和色度的方式有助于视频编码和压缩。

  > 因为人眼对亮度变化更敏感，而对颜色变化的敏感度较低。
  >
  > 因此，通过将图像的色度信息分开编码，减少色度采样，可以降低数据量同时保持图像质量。

- 最常用的表示形式是Y、U、V都使用8个bit来表示

  - 取值范围就是``0～255``

    > 为了防止信号变动造成过载，在广播电视系统中不传输很低和很高的数值。把边界的数值作为“保护带”。
    >
    > 不论是Rec.601还是BT.709的广播电视标准中，Y的取值范围都是16～235, UV的取值范围都是16～240。

- 很早之前在绘画界就有了先用灰色颜料描绘亮度变化，然后再上色的技法

  - 这与灰度图叠加色度图的原理是一致的

​		

​		YCbCr 其实是 YUV 经过缩放和偏移的翻版。其中 Y 与 YUV 中的 Y 含义一致, Cb，Cr 同样都指色彩, 只是在表示方法上不同而已。在 YUV 家族中, YCbCr 是在计算机系统中应用最多的成员，其应用领域很广泛，JPEG、MPEG 均采用此格式。一般人们所讲的 YUV 大多是指YCbCr。

-  **Cb（相当于U ，blue）：**反映的是 RGB 输入信号蓝色部分与 RGB 信号亮度值之间的差异。
-  **Cr（相当于V ，red）：**反映了 RGB 输入信号红色部分与 RGB 信号亮度值之间的差异。

```c
\\ 公式 并不是固定的， 不同的RGB的格式，转换的的公式也是不一样的
\\ RGB 转换为 Ycbcr 公式

Y = 0.257*R+0.564*G+0.098*B+16
Cb = -0.148*R-0.291*G+0.439*B+128
Cr = 0.439*R-0.368*G-0.071*B+128

\\ Ycbcr 转换为 RGB 公式

R = 1.164*(Y-16)+1.596*(Cr-128)
G = 1.164*(Y-16)-0.392*(Cb-128)-0.813*(Cr-128)
B = 1.164*(Y-16)+2.017*(Cb-128)
```

​		**使用YUV的优点有两个:** 

- 彩色YUV图像兼容黑白YUV图像。 只保留Y亮度分量就是黑白灰度图像。
- YUV是数据总尺寸小于RGB格式
  - 减少颜色分量，减少体积



#### YUV的采样格式

​		YUV采用`A:B:C`表示法来描述Y、U、V采样频率的关系。

​		对非压缩的`8 bit`量化的采样数据来说，每个分量占一个字节（char或byte）。为节省带宽，大多数 YUV 格式平均使用的每像素位数都少于`24 bit`。

​		YUV采样时，将水平连续两个像素，垂直两行的两个像素，共4个像素看做一个宏像素块（macro-pixel)来进行共同采样，其中每个像素中都采集亮度分量Y，色度分量UV（cr/cb，红色分量和蓝色分量）的同比例采样，但与亮度分量的采样比例不同。从而产生了不同的YUV的数据存储格式。

![image-20240828223910705](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20240828223910705.png)

- `YUV444`：`(4*8 + 4*8 + 4*8) / 4 = 24 bit`
  - 每个像素中都采集色度分量
  - 和`RGB`大小一样
- `YUV422`：`(4*8 + 2*8 + 2*8) / 4 = 16 bit`
  - `2:1`水平色度采样，即水平的两个像素中采集一个的色度，即隔列采集
  - `1:1`垂直色度采样，即垂直的两个像素都采集色度，即按行采集
  - 比RGB小1/3
- `YUV420`：`(4*8 + 1*8 + 1*8) / 4 = 12 bit`
  - `2:1`水平色度采样，即水平的两个像素中采集一个的色度，即隔列采集
  - `2:1`垂直色度采样，即垂直的两个像素中采集一个的色度，即隔行采集
  - 比RGB小半



#### YUV的存储格式

##### packed 打包格式

​		也称交错模式，每个像素点为单位的Y、U、V分量视作一个`packed`，以像素点为单元连续存储数据。

> ​		之所以称为交错模式，是因为不同采样格式下，YUV的数据量不同，亮度色度要进行的处理也不同，但存储时不加说明的以流式写入同一存储，需要读取时交错读取处理。

![image-20220908211732290](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908211732290.png)

##### planar 平面格式

​		将Y、U、V分量数据分别存储在不同的存储块中。

> 不同格式处理时，只需按比例从不同的存储中读出相应大小的数据即可恢复像素点。															

![image-20220908211748811](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908211748811.png)

![image-20220908221051838](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908221051838.png)



##### 混合模式

​		因为先存储哪个分量并没有关系，所以根据存储顺序有着不同的存储格式，以YUV420举例：

- `YUV420P`平面格式，能形成三个组
  - `I420：YYYYYYYY UU VV`
  - `YV12：YYYYYYYY VV UU`
- `YUV420SP`打包格式，一般只会将`UV`交错，而`Y`按序存储
  - `NV12：YYYYYYYY UVUV`
  - `NV21：YYYYYYYY VUVU`





![`I420`](https://raw.githubusercontent.com/Mocearan/picgo-server/main/v2-ab9144880c191266bb26dda9445538d6_1440w.webp)



#### YUV和RGB的转化

​		凡是渲染到屏幕上的东西（文字、图片或者其他），都要转换为RGB的表示形式。

​		通常情况下RGB和YUV直接的相互转换都是调用接口实现，比如`Ffmpeg`的`swscale`或者`libyuv`等库。主要转换标准是 BT601 和 BT709。

![image-20220908221221048](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908221221048.png)

​		YUV(256 级别) 可以从8位 RGB 直接计算：

```
Y = 0.299R + 0.587G + 0.114B
U = -0.169R - 0.331*G + 0.5B
V = 0.5 R - 0.419G - 0.081B
```

8bit位深的情况下，TV range是16-235(Y)、16-240(UV) , 也叫Limited Range；PC range是0-255，也叫Full Range。而RGB没有range之分，全是0-255。

​		反过来，RGB 也可以直接从YUV (256级别) 计算：

```
R = Y + 1.402 (Y-128)
G = Y - 0.34414 (U-128) - 0.71414 (U-128)
B = Y + 1.772 (V-128)
```

- 从YUV转到RGB，如果值小于0要取0，大于255要取255.



​		比较典型的场景：

> ​		iOS平台中使用摄像头采集出YUV数据之后，上传显卡成为一个纹理ID，这个时候就需要做YUV到RGB的转换（具体的细节会在后面的章节中详细讲解）。在iOS的摄像头采集出一帧数据之后（CMSampleBufferRef），我们可以在其中调用CVBufferGetAttachment来获取YCbCrMatrix，用于决定使用哪一个矩阵进行转换。
>
> ​		对于Android的摄像头，由于其是直接纹理ID的回调，所以不涉及这个问题。
>
> ​		其他场景下需要自行寻找对应的文档，以找出适合的转换矩阵进行转换。

为什么解码出错显示绿屏？

> ​		因为解码失败时YUV分量都填为0值，然后根据公式：R = 1.402 * (-128) = -126.598，G = -0.34414 * (-128) - 0.71414 * (-128) = 44.04992 + 91.40992 = 135.45984，B = 1.772 * (-128) = -126.228。
> ​		RGB 值范围为[0，255]， 所以最终的值为：R = 0，G = 135.45984，B = 0。
> ​		此时只有G分量有值所以为绿色。





## 基本概念

- 色彩空间：`RGB/YUV`，是像素点记录色彩数据的方式

- 像素（pixel）：像素是构成图片的基本单位，每个像素就是一个视觉采样的单元

  - 像素由RGB或YUV色彩空间构成

- 分辨率：是指图像的大小或尺寸，同时表达了图片的采样量

  - 比如`1920x1080`表示横`1920`个像素，纵`1080`个像素的排列，两百万像素
  - 分辨率的比例一般是`16:9`
  - 分辨率后面跟`P`表示为逐行扫描；跟`I`为隔行扫描
    - 隔行扫描一般是电视使用，可能出现锯齿
    - 逐行扫描一般是网络、视频文件使用

- DPI：每英寸显示设备上的像素数量

  - 由设备分辨率决定

- 位深：是指在采样视觉数据时，用来描述数字图像颜色数据的数据量
  - 比如RGB中红色分量用`8bit`，YUV中Y分量用`8bit`
  - RGB中用红绿蓝三原色加上透明度alpha分量来描述一个像素，位深越大描述的像素颜色越多
  - YUV中用亮度（Y）、色度描述（U和V）一个像素，位深越大画面越细腻

- 跨距（Stride）：内存中每行像素所占的空间
  - 编码中存在内存对齐导致每行像素在内存中的数据表示和理论上的图像宽度并不一定相同

  - 也称为Padding，如果图像的每一行像素末尾拥有对其内容，Stride 的值一定大于图像的宽度值

  - ![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/8705ced6a6764833ab4ba32cbd14ef56.png)

    > 分辨率638x480的RGB24图像，内存处理的时候如果要以16字节对齐
    >
    > ​	638/16=119.625不能整除，需要在每行尾部填充6个字节：`640 / 16 = 120 /638->640 / 2*24bit=48bit=6byte `。
    >
    > ​	此时该图片的stride为``480*6=1920``字节。





- 画质：画面质量，由清晰度、锐度、解析度、色彩纯度、色彩平衡等指标构成。
- 清晰度：指图像细节纹理及其边界的清晰程度。
- 锐度：反应图像平面清晰程度，以及图像边缘的锐利程度。
- 解析度：指像素点的数量，与分辨率对应，分辨率越高，解析度越高。
- 色彩纯度：指色彩的鲜艳程度。所有色彩都是三原色组成，其他颜色都是三原色混合而成，理论上可以混合出256种颜色。原色的纯度最高。色彩纯度是指原色在色彩中的百分比。
- 色彩平衡：用来控制图像的色彩分布，使得图像整体达到色彩平衡。
- 色域：指某种表色模式所能表达的颜色构成的范围区域，色域空间越大，所能表现的颜色越多。
- HDR：High Danamic Range，高动态范围，比普通图像提供更多动态范围和图像细节，能够更好反应真实环境的视觉效果。颜色值经过归一化后，范围一般是[0,1]。而HDR可以表达超出1的颜色值，拥有更大的颜色范围。
- 旋转角度：视频的YUV储存方向。一般的视频旋转角度是0°，对应的是横屏显示。后置摄像头竖屏拍的视频，旋转角度为90°，对应的是竖屏显示。Android中可以通过MediaMetaDataRetriever获取旋转角度。
- 时长：视频所有图像播放所需要的时间称为视频时长。计算公式：时长(s)=帧数x每帧时长=帧数x(1/帧率)。假设一个视频帧数为1000，帧率为25fps，那么时长为40s。





## 图像的编码：压缩

​		由于图像裸数据很大，一般对图像进行压缩。

### JPEG

​		静态图像压缩标准，ISO指定。

- 良好的压缩性能

- 较好的重建质量

- 图像处理领域

- 有损压缩

- 不能直接用于视频压缩

  > 视频需要考虑时域因素，不仅要考虑帧内编码，还要考虑帧间编码。（帧，视频中的一幅图像）

