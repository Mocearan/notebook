# media base

---



## Audio & Video（Image） 声与像

### Audio



#### 声音的物理性质

- 声音是波

  - 频率：音阶， 频率越高，波长越短，穿透性越差，衰减越快。

    > 人耳范围20Hz~20KHz，对3kHz~4kHz最为敏感

  - 振幅：响度，能量大小的反映，常用分贝描述。

    > 在声压级较高时，听觉的频率特性会变得较为均匀。频率范围较宽的音乐，其声压以80～90dB为最佳，超过90dB将会损害人耳（105dB为人耳极限）

  - 波形：音色，不同音色是因为不同介质产生的波形不同。

- 介质

  - 发声介质
  - 传播介质

  > 吸音主要是解决声音反射而产生的嘈杂感，录音棚
  >
  > 隔音主要是解决声音的透射而降低主体空间内的吵闹感，KTV

- 回声

  ​	声音在传播过程中遇到障碍物会反弹回来，再次被我们听到。

  ​	两种声音传到我们的耳朵里的时差小于80毫秒，我们就无法区分开这两种声音。

- 共鸣

  ​	声波能量引起另一物体振动，从而以同样的频率产生声音。

#### 声音的采集

​		麦克风里面有一层碳膜，非常薄而且十分敏感。1.1节中介绍过，声音其实是一种纵波，会压缩空气也会压缩这层碳膜，碳膜在受到挤压时也会发出振动，在碳膜的下方就是一个电极，碳膜在振动的时候会接触电极，接触时间的长短和频率与声波的振动幅度和频率有关，这样就完成了声音信号到电信号的转换。之后再经过放大电路处理，就可以实施后面的采样量化处理了。

​		即将物理的声音转为电模拟信号。

#### 数字音频

- 采样：在时间轴上对信号进行数字化。

  > ​		对模拟信号进行采样。
  >
  > ​		奈奎斯特定理（也称为采样定理），按比声音最高频率高2倍以上的频率对声音进行采样（也称为AD转换）,1.1节中提到过，对于高质量的音频信号，其频率范围（人耳能够听到的频率范围）是20Hz～20kHz，所以采样频率一般为44.1kHz，这样就可以保证采样声音达到20kHz也能被数字化，从而使得经过数字化处理之后，人耳听到的声音质量不会被降低。而所谓的44.1kHz就是代表1秒会采样44100次（如图1-5所示）。

- 量化：在幅度轴上对信号进行数字化

  > ​		比如用16比特的二进制信号来表示声音的一个采样，而16比特（一个short）所表示的范围是[-32768,32767]，共有65536个可能取值，因此最终模拟的音频信号在幅度上也分为了65536层

- 编码：按照一定的格式记录采样和量化后的数字数据，比如顺序存储或压缩存储

  > ​		通常所说的音频的裸数据格式就是脉冲编码调制（Pulse Code Modulation, PCM）数据。

  ​		编码后的二进制数据即表示将模拟信号转为数字信号。



#### 音频编码

​		将实际量化后的数字音频数据进行压缩存储，称为编码。

​		压缩编码的原理实际上是压缩掉人耳不能感知的冗余信号，包括人耳听觉范围之外的音频信号以及被掩蔽掉的音频信号等。

> 人耳掩蔽效应：频域掩蔽效应和时域掩蔽效应。

- 有损压缩

  ​	解压后的数据不能完全复原，压缩比越小，丢失的信息越多，信号还原后的失真就会越大。

- 无损压缩

  ​	解压后的数据可以完全复原。

​		根据不同的应用场景（包括存储设备、传输网络环境、播放设备等），可以选用不同的压缩编码算法，如``PCM、WAV、AAC、MP3、Ogg``等。



##### PCM

​		脉冲编码调制。

​		描述一段PCM数据一般需要以下几个概念：量化格式（sampleFormat）、采样率（sampleRate）、声道数（channel）。

​		以CD的音质为例：量化格式（有的地方描述为位深度）为16比特（2字节），采样率为44100，声道数为2，这些信息就描述了CD的音质。对于声音格式，还有一个概念用来描述它的大小，称为数据比特率，即1秒时间内的比特数目，它用于衡量音频数据单位时间内的容量大小：``比特率 = 采样率 * 位深度 * 声道数``

> ​		而对于CD音质的数据，比特率为多少呢？计算如下：44100 ＊ 16 ＊ 2 = 1378.125kbps那么在1分钟里，这类CD音质的数据需要占据多大的存储空间呢？计算如下：1378.125 ＊ 60 / 8 / 1024 = 10.09MB当然，如果sampleFormat更加精确（比如用4字节来描述一个采样），或者sampleRate更加密集（比如48kHz的采样率），那么所占的存储空间就会更大，同时能够描述的声音细节就会越精确。

##### WAV编码

​		WAV编码有多种实现，但是否不会进行压缩操作。

​		其中一种实现，是在PCM数据格式的前面加上44字节，分别用来描述PCM的采样率、声道数、数据格式等信息。

- 音质好
- 大量软件支持
- 适用于多媒体开发中的中间文件、保存音乐和音效素材



##### MP3

​		MP3的压缩比不错，其中一种实现为`LAME`编码。

​		使用`LAME`编码的中高码率的MP3文件，听感上非常接近源WAV文件，不同的应用场景下，应该调整合适的参数以达到最好的效果。

- 音质在128Kbit/s以上表现还不错
- 压缩比较高，大量软件和硬件都支持，兼容性好
- 使用与高比特率下对兼容性有要求的音乐欣赏



##### AAC编码

​		AAC是新一代的音频有损压缩技术，通过一些附加的编码技术（PS，SBR等），衍生出了

- LC-AAC

  ​	比较传统的AAC，主要应用于中高码率场景的编码（>80Kbit/s)

- HE-AAC

  ​	AAC + SBR， 主要应用于中低码率场景下的编码（<=80Kbit/s)

- HE-AAC v2

  ​	AAC + SBR + PS，主要应用于低码率场景的编码(<=48Kbit/s)

  

  大部分编码器都设置为<=48Kbit/s自动启用PS技术，而>48Kbit/s则不加PS。

- 小于128Kbit/s的码率下表现优异，并且多用于视频中的音频编码

- 适用于128Kbit/s以下的音频编码，多用于视频总的音频轨的编码



##### Ogg编码

​		Ogg是一种非常有潜力的编码，在各种码率下都有比较优秀的表现，尤其是在中低码率场景下，音质好，且免费。

​		Ogg有着非常出色的算法，可以用更小的码率达到更好音质，128Kbit/s的Ogg比192Kbit/s甚至更高码率的MP3还要出色。但目前没有媒体服务软件支持，因此基于Ogg的数字广播还无法实现。

- 可以用比MP3更小的码率实现比MP3更好的音质，高中低码率下均有良好的表现，兼容性不够好，流媒体特性不支持。.
- 适用场合：语音聊天的音频消息场景



### Image

#### 图像的物理性质

​		白光能被分解为三原色，红光（R）、绿光（G）、蓝光（B），即RGB。等量的三原色光叠加会变为白光。

​		电子设备显像是自发光的，不同于其他自然物体是由于反射日光。

#### 电子设备图像表示：RGB

​		像素(pixel)是一个图片的基本单位。

> pix是picture的缩写，el是element的缩写，像素即图像元素。

​		一个像素点由RGB三个通道组成，每个通道按照一定位数的值，显示不等量的对应三原色，从而构成该像素点的颜色。若干个颜色各异的像素点就组成一幅图，连续变换的一组图就形成一个视频。

​		RGB表示上，除了红绿蓝三个色彩通道，还有一个透明度通道（alpha分量， ARGB）。

- 分辨率，描述一个现象设备上的像素点数，即像素的大小或尺寸。

  > 如1920*1080，即横向1920个像素点，纵向1080个像素点。
  >
  > 分辨率越高， 像素越多，图像就越清晰。

- 位深，计算机表示数字图像时，每个像素用一定位数的数据长度来表示。

  ​	每个通道的位深越大，能够表示的颜色值就越多。

  > 每个通道上，按照一定的规则来定义对应颜色的分级。
  >
  > RGB通道的分级数值表示组成一个像素点的数据格式：
  >
  > - 浮点表示：0/0~10，OpenGL ES中每个通道点的表示使用这种方式。
  >
  > - 整数表示：RGB_8888,0~255或00~FF,8位表示一个分量通道上的值。即8位位深度。
  >
  >   > ​	一些平台上采用其他的整数表示方式，如Android采用RGB_565共16位比特来表示一个像素。
  >  >
  >   > RGB_8888表示的1280X720的图像大小为：1280X720X4=3.516MB
  >   >
  >   > ​		这也是位图（bitmap）在内存中所占用的大小，所以每一张图像的裸数据都是很大的。
  > 
  
  
  

​		RGB像素点中各个分量通常是按照顺序排列的，但有些图像处理要转换成其他的顺序，`opencv`经常转换成BGR的排列方式。

![image-20220907235455776](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220907235455776.png)

#### 图像的编码：压缩

​		由于图像裸数据很大，一般对图像进行压缩。

##### JPEG

​		静态图像压缩标准，ISO指定。

- 良好的压缩性能

- 较好的重建质量

- 图像处理领域

- 有损压缩

- 不能直接用于视频压缩

  > 视频需要考虑时域因素，不仅要考虑帧内编码，还要考虑帧间编码。（帧，视频中的一幅图像）



### video

​		视频是由一幅幅图像组成的。

#### 电视图像数字表示：YUV

​		视频帧的裸数据表示，更多的是YUV数据格式，主要应用于优化彩色视频信号的传输，使其向后兼容老式黑白电视。

​		之所以采用YUV色彩空间，是因为它的亮度信号Y和色度信号U、V是分离的。

> ​		如果只有Y信号分量而没有U、V分量，那么这样表示的图像就是黑白灰度图像。
>
> ​		彩色电视采用YUV空间正是为了用亮度信号Y解决彩色电视机与黑白电视机的兼容问题，使黑白电视机也能接收彩色电视信号。
>
> ​		同时降低色度的采样率而不会对图像质量影响太大，降低了视频信号传输时对频宽（带宽）的要求。

- Y，明亮度（Luminance或Luma），也称灰阶值

  > “亮度”是透过RGB输入信号来建立的，方法是将RGB信号的特定部分叠加到一起。

- U / V，色度（Chrominance或Chroma），它们的作用是描述影像的色彩及饱和度，用于指定像素的颜色。

  > ​	“色度”则定义了颜色的两个方面——色调与饱和度，分别用Cr和Cb来表示。
  >
  > 其中
  >
  > - Cr反映了RGB输入信号红色部分与RGB信号亮度值之间的差异
  > - Cb反映的则是RGB输入信号蓝色部分与RGB信号亮度值之间的差异。

​		最常用的表示形式是Y、U、V都使用8个bit来表示，所以取值范围就是0～255。

> ​		在广播电视系统中不传输很低和很高的数值，实际上是为了防止信号变动造成过载，因而把这“两边”的数值作为“保护带”，不论是Rec.601还是BT.709的广播电视标准中，Y的取值范围都是16～235, UV的取值范围都是16～240。

##### YUV的采样格式

​		YUV采用`A:B:C`表示法来描述Y、U、V采样频率的关系。

​		主要分为YUV 4:4:4 、YUV 4:2:2、YUV 4:2:0三种常用类型。

![image-20220908213114325](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908213114325.png)

> ​		黑点表示一个Y分量，空心圆表示像素点的一个UV分量组合。
>
> ​		YUV最常用的采样格式是4:2:0,它指的是对每行扫描线来说，只有一种色度分量是以2:1的抽样率来存储的。
>
> ​		相邻的扫描行存储着不同的色度分量，也就是说，如果某一行是4:2:0，那么其下一行就是4:0:2，再下一行是4:2:0，以此类推。
>
> ​		对于每个色度分量来说，水平方向和竖直方向的抽样率都是2:1，所以可以说色度的抽样率是4:1。
>
> > 对非压缩的8比特量化的视频来说，8×4的一张图片需要占用48字节的内存：
> >
> > ![image-20220821230236373](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220821230236373.png)
> >
> > 上述描述意思是，对于一组四个Y，在U/V分量中采一个U和一个V。

##### YUV的排列格式

​		YUV是一系列相似格式的统称，针对具体的排列方式，可以分为多种格式。

- 打包（packed）格式

  ​	每个像素点的Y、U、V分量交叉排列，易像素点为单元连续存放在同一数组中。通常几个相邻的像素组成一个宏像素（macro-pixel)

  ![image-20220908211732290](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908211732290.png)

- 平面（planar）格式

  ​	将Y、U、V分量分别各自连续的存放在不同的数组中。

![image-20220908211748811](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908211748811.png)

![image-20220908221051838](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908221051838.png)

##### YUV的数据存储

​		对非压缩的8比特量化的采样数据来说，每个分量占一个字节（char或byte）

- YUV 4:4:4

  ​	I444格式（YUV444P, P-planar），对于ffmpeg像素表示`AV_PIX_FMT_YUV444P`

  > planar YUV4:4:4， 24bpp(24bit per pixel)，（1Cr & Cb sample per 1X1 Y samples）
  >
  > > Y 4byte + U 4byte + V 4byte = 12byte
  > >
  > > 即4个像素占用12byte，每个像素占用3byte

  ![image-20220908214343304](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908214343304.png)

- YUV 4:2:2

  ​	I422 格式（YUV422P），对于ffmpeg像素表示`AV_PIX_FMT_YUV422P`

  > planar YUV4:2:2， 16bpp(16bit per pixel)，（1Cr & Cb sample per 2X1 Y samples）
  >
  > > Y 4byte + U 2byte + V 2byte = 8byte
  > >
  > > 即4个像素占用8byte，每个像素占用2byte

  ![image-20220908214659406](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908214659406.png)

- YUV 4:2:0

​	I420格式（YUV420P），对于ffmpeg像素表示`AV_PIX_FMT_YUV420P`

> planar YUV4:2:0， 12bpp(12bit per pixel)，（1Cr & Cb sample per 2X2 Y samples）
>
> > Y 4byte + U 1byte + V 1byte = 6byte
> >
> > 即4个像素占用6byte，每个像素占用1.5byte

![image-20220908215935314](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908215935314.png)

##### YUV和RGB的转化

​		凡是渲染到屏幕上的东西（文字、图片或者其他），都要转换为RGB的表示形式。

​		主要的转换标准是BT601和BT709.

![image-20220908221221048](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908221221048.png)

![image-20220821232507499](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220821232507499.png)

​	![image-20220908221533694](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908221533694.png)

> 比较典型的场景：
>
> ​		iOS平台中使用摄像头采集出YUV数据之后，上传显卡成为一个纹理ID，这个时候就需要做YUV到RGB的转换（具体的细节会在后面的章节中详细讲解）。在iOS的摄像头采集出一帧数据之后（CMSampleBufferRef），我们可以在其中调用CVBufferGetAttachment来获取YCbCrMatrix，用于决定使用哪一个矩阵进行转换。
>
> ​		对于Android的摄像头，由于其是直接纹理ID的回调，所以不涉及这个问题。
>
> 其他场景下需要自行寻找对应的文档，以找出适合的转换矩阵进行转换。

​		ffmpeg提供swscale或者libyuv进行RGB和YUV的转换。

- 从YUV转到RGB，如果值小于0要取0，大于255要取255.

![image-20220908221700552](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908221700552.png)

#### 视频编码

​		同音频编码相似，视频的帧间编码也是通过去除冗余信息的方式来进行数据量的压缩。

​		相较于音频数据，视频数据有极强的相关性，也就是说有大量的冗余信息，包括空间上的冗余信息和时间上的冗余信息。

![image-20220821235406691](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220821235406691.png)

**帧间编码**技术可以去除时间上的冗余信息

- 运动补偿：运动补偿是通过先前的局部图像来预测、补偿当前的局部图像，它是减少帧序列冗余信息的有效方法。
- 运动表示：不同区域的图像需要使用不同的运动矢量来描述运动信息。
- 运动估计：运动估计是从视频序列中抽取运动信息的一整套技术。

**帧内编码**技术可以去除空间上的冗余信息

​		相对于静态的JPEG编码，ISO针对视频也指定了标准：Motion JPEG即MPEG。

​		MPEG算法是适用于动态视频的压缩算法，它除了对单幅图像进行编码外，还利用图像序列中的相关原则去除冗余，这样可以大大提高视频的压缩比。

> 截至目前，MPEG的版本一直在不断更新中，主要包括这样几个版本：Mpeg1（用于VCD）、Mpeg2（用于DVD）、Mpeg4 AVC（现在流媒体使用最多的就是它了）

​		ITU-T制定的H.261、H.262、H.263、H.264一系列视频编码标准是一套单独的体系。

​		H.264集中了以往标准的所有优点，并吸取了以往标准的经验，采用的是简洁设计，这使得它比Mpeg4更容易推广。现在使用最多的就是H.264标准，H.264创造了多参考帧、多块类型、整数变换、帧内预测等新的压缩技术，使用了更精细的分像素运动矢量（1/4、1/8）和新一代的环路滤波器，这使得压缩性能得到大大提高，系统也变得更加完善。



##### IPB帧

​		视频压缩中，每帧都代表着一幅静止的图像。而在进行实际压缩时，会采取各种算法以减少数据的容量，其中IPB帧就是最常见的一种。

- I帧：帧内编码帧（intra picture）

  I帧通常是每个GOP的第一个帧，经过适度地压缩，作为随机访问的参考点，可以当成静态图像。

  > GOP（MPEG所使用的一种视频压缩技术）

  I帧可以看作一个图像经过压缩后的产物，I帧压缩可以得到6:1的压缩比而不会产生任何可觉察的模糊现象。

  I帧压缩可去掉视频的空间冗余信息，P帧和B帧是为了去掉时间冗余信息。

  > I帧自身可以通过视频解压算法解压成一张单独的完整视频画面，所以I帧去掉的是视频帧在空间维度上的冗余信息。

- P帧：前向预测编码帧（predictive-frame）

  通过将图像序列中前面已编码帧的时间冗余信息充分去除来压缩传输数据量的编码图像，也称为预测帧。

  > P帧需要参考其前面的一个I帧或者P帧来解码成一张完整的视频画面。

- B帧：双向预测内插编码帧（bi-directional interpolated prediction frame）

  既考虑源图像序列前面的已编码帧，又顾及源图像序列后面的已编码帧之间的时间冗余信息，来压缩传输数据量的编码图像，也称为双向预测帧。

  > B帧则需要参考其前一个I帧或者P帧及其后面的一个P帧来生成一张完整的视频画面，所以P帧与B帧去掉的是视频帧在时间维度上的冗余信息。

##### IDR帧

​		在H264的概念中有一个帧称为IDR帧（在H264的概念中有一个帧称为IDR帧）。

​		因为H264采用了多帧预测，所以I帧之后的P帧有可能会参考I帧之前的帧，这就使得在随机访问的时候不能以找到I帧作为参考条件，因为即使找到I帧，I帧之后的帧还是有可能解析不出来，而IDR帧就是一种特殊的I帧，即这一帧之后的所有参考帧只会参考到这个IDR帧，而不会再参考前面的帧。在解码器中，一旦收到一个IDR帧，就会立即清理参考帧缓冲区，并将IDR帧作为被参考的帧。



##### PTS / DTS

​		DTS（Decoding Time Stamp），解码时间戳，主要用于视频的解码。

> 表示该压缩包应该在什么时候被解码

​		PTS（Presentation Time Stamp），显示（表示）时间戳，主要用于在解码阶段进行视频的同步和输出。

> 视频的一帧图像什么时候显示给用户，取决于它的PTS

​		在没有B帧的情况下，DTS和PTS的输出顺序是一样的。因为B帧打乱了解码和显示的顺序，所以一旦存在B帧，PTS与DTS势必就会不同。

> 如果视频里各帧的编码是按输入顺序（显示顺序）依次进行的，那么解码和显示时间应该是一致的，但是事实上，在大多数编解码标准（如H.264或HEVC）中，编码顺序和输入顺序并不一致，于是才会需要PTS和DTS这两种不同的时间戳。



##### GOP

​		GOP（Group Of Picture），两个I帧之间形成的一组图片。

​		通常在为编码器设置参数的时候，必须要设置gop_size的值，其代表的是两个I帧之间的帧数目。

> ​		一个GOP中容量最大的帧就是I帧，所以相对来讲，gop_size设置得越大，整个画面的质量就会越好。
>
> ​		但是在解码端必须从接收到的第一个I帧开始才可以正确解码出原始图像，否则会无法正确解码。

​		在提高视频质量的技巧中，还有个技巧是多使用B帧，一般来说，I的压缩率是7（与JPEG差不多）, P是20, B可以达到50，可见使用B帧能节省大量空间，节省出来的空间可以用来更多地保存I帧，这样就能在相同的码率下提供更好的画质。

> 根据不同的业务场景，适当地设置gop_size的大小，以得到更高质量的视频。

##### 帧率

​		每秒处理的图片帧数，如25fps，即每秒处理25张图片。

​		帧率越高，视频越流畅，需要的设备性能越高。

​		人眼的视觉暂留使得图像帧率达到24帧就可以认为图像是连续的。

> 电影帧率一般是24fps，电视剧是25fps，监控行业常用25fps，音视频通话常用15fps。

##### 码率

​		单位时间的视频数据大小，如1Mbps，即每秒有1Mb的数据量。

​		码率越大，视频越清晰。但不是线性的关系，而是对数的关系。



##### Stride

​		内存中每行像素所占的空间，为了实现内存对齐，每行像素在内存中所占的空间并不一定是图像的宽度。

![image-20220908221919639](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908221919639.png)

![image-20220908222150926](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908222150926.png)

![image-20220908222203828](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220908222203828.png)

## 流媒体基本原理

### 录制

![image-20220816211216411](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220816211216411.png)

### 播放

![image-20220816211236450](https://raw.githubusercontent.com/Mocearan/picgo-server/main/image-20220816211236450.png)



















